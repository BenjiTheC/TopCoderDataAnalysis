{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.curdir, 'data', 'detail_requirements.json')) as f:\n",
    "    detailed_req = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_txt_from_node(node, delimiter=' '):\n",
    "    \"\"\" Extract text and lowercase the text then unify the white space.\"\"\"\n",
    "    return delimiter.join(node.get_text().lower().split())\n",
    "\n",
    "\n",
    "def sectionlize_requirements(req):\n",
    "    \"\"\" Aggregate the requirement paragraph by header tag. \"\"\"\n",
    "    sectioned_req_dct = defaultdict(list)\n",
    "    soup = BeautifulSoup(req, 'html.parser')\n",
    "    \n",
    "    all_header_tags = soup.find_all(re.compile(r'^h'))\n",
    "    \n",
    "    if len(all_header_tags) == 0:\n",
    "        return {'no_header_tag': extract_txt_from_node(soup)}\n",
    "    \n",
    "    for header in all_header_tags:\n",
    "        section_name = extract_txt_from_node(header, delimiter='_')\n",
    "        nxt_node = header\n",
    "        while True:\n",
    "            nxt_node = nxt_node.nextSibling\n",
    "            \n",
    "            if nxt_node is None:\n",
    "                break\n",
    "                \n",
    "            if isinstance(nxt_node, NavigableString):\n",
    "                sectioned_req_dct[section_name].append(nxt_node.strip())\n",
    "            if isinstance(nxt_node, Tag):\n",
    "                if nxt_node.name.startswith('h'):\n",
    "                    break\n",
    "                sectioned_req_dct[section_name].append(extract_txt_from_node(nxt_node))\n",
    "    \n",
    "    return {sec_name: ' '.join(' '.join(sec_reqs).split()) for sec_name, sec_reqs in sectioned_req_dct.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_req = defaultdict(dict)\n",
    "\n",
    "for dr in detailed_req:\n",
    "    processed_req[dr['project_id']][dr['challenge_id']] = {\n",
    "        'title': ' '.join(dr['title'].lower().split()),\n",
    "        'requirements': sectionlize_requirements(dr['requirements'])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenv5b62939097214b20b0ca6487fffe5cee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
