{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.curdir, 'data', 'detail_requirements.json')) as f:\n",
    "    detailed_req = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_txt_from_node(node, delimiter=' '):\n",
    "    \"\"\" Extract text and lowercase the text then unify the white space.\"\"\"\n",
    "    return delimiter.join(node.get_text().lower().split())\n",
    "\n",
    "\n",
    "def sectionlize_requirements(req):\n",
    "    \"\"\" Aggregate the requirement paragraph by header tag. \"\"\"\n",
    "    sectioned_req_dct = defaultdict(list)\n",
    "    soup = BeautifulSoup(req, 'html.parser')\n",
    "    \n",
    "    all_header_tags = soup.find_all(re.compile(r'^h'))\n",
    "    \n",
    "    if len(all_header_tags) == 0:\n",
    "        return {'no_header_tag': extract_txt_from_node(soup)}\n",
    "    \n",
    "    for header in all_header_tags:\n",
    "        section_name = extract_txt_from_node(header, delimiter='_')\n",
    "        nxt_node = header\n",
    "        while True:\n",
    "            nxt_node = nxt_node.nextSibling\n",
    "            \n",
    "            if nxt_node is None:\n",
    "                break\n",
    "                \n",
    "            if isinstance(nxt_node, NavigableString):\n",
    "                sectioned_req_dct[section_name].append(nxt_node.strip())\n",
    "            if isinstance(nxt_node, Tag):\n",
    "                if nxt_node.name.startswith('h'):\n",
    "                    break\n",
    "                sectioned_req_dct[section_name].append(extract_txt_from_node(nxt_node))\n",
    "    \n",
    "    return {sec_name: ' '.join(' '.join(sec_reqs).split()) for sec_name, sec_reqs in sectioned_req_dct.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_req = defaultdict(dict)\n",
    "\n",
    "for dr in detailed_req:\n",
    "    processed_req[dr['project_id']][dr['challenge_id']] = {\n",
    "        'title': ' '.join(dr['title'].lower().split()),\n",
    "        'requirements': sectionlize_requirements(dr['requirements'])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_by_proj = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for project_id, challenges in processed_req.items():\n",
    "    common_section_names = set.intersection(*[set(challenge['requirements'].keys()) for challenge in challenges.values()])\n",
    "    \n",
    "    for section_name in common_section_names:\n",
    "        if section_name != 'no_header_tag':\n",
    "            sections_by_proj[project_id][section_name] = [challenge['requirements'][section_name] for challenge in challenges.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(lst_of_str):\n",
    "    \"\"\" Calculate the simliarity scroe from a list of strings\"\"\"\n",
    "    seq_matcher = difflib.SequenceMatcher()\n",
    "    similarity_score_sum = 0\n",
    "    \n",
    "    for idx, s in enumerate(lst_of_str[:-1]):\n",
    "        seq_matcher.set_seq2(s)\n",
    "        for s1 in lst_of_str[idx + 1:]:\n",
    "            seq_matcher.set_seq1(s1)\n",
    "            similarity_score_sum += round(seq_matcher.ratio(), 3)\n",
    "            \n",
    "    return round(similarity_score_sum / ((len(lst_of_str) * (len(lst_of_str) - 1)) / 2), 3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_similarity_score = defaultdict(dict)\n",
    "\n",
    "for project_id, requirement_section in sections_by_proj.items():\n",
    "    for sec_name, lst_of_requirements in requirement_section.items():\n",
    "        section_similarity_score[project_id][sec_name] = get_similarity_score(lst_of_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenv5b62939097214b20b0ca6487fffe5cee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
