{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This notebook will be the formal to train, analyze the word embedding data\n",
    "    (with some ugly code temperately existed of course - but will be cleaned eventually!)\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tc_main import TopCoder\n",
    "from tc_pricing_models import train_word2vec_model, reduce_wv_dimensions, plot_word2vec, cosine_similarity, doc_vector_from_word_vectors\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "TRAINED_WV_PATH = os.path.join(os.curdir, 'models', 'model_20200510T145859')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topcoder = TopCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment following line if training a new model\n",
    "# trained_wv = train_word2vec_model(sentences=topcoder.corpus.get_challenge_req_sentences())\n",
    "\n",
    "# Un-comment following line if using a trained model\n",
    "trained_wv = KeyedVectors.load(TRAINED_WV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of 2-element combinations of `cha_vec_dct` with replacement is 25336521, each execution of `cosine_similarity` takes 15.3 µs ± 1.74 µs (`timeit` with 100 runs and 10000 loops per run). To calculate all cosine similarity for every pair of documents that's stored in a default dict\n",
    "\n",
    "```python\n",
    "cha_cos_sim_matrix = defaultdict(dict)\n",
    "for cha_a, cha_b in itertools.combinations_with_replacement(cha_vec_dct.keys(), 2):\n",
    "    cha_cos_sim_matrix[cha_a][cha_b] = cha_cos_sim_matrix[cha_b][cha_a] = cosine_similarity(cha_vec_dct[cha_a], cha_vec_dct[cha_b])\n",
    "```\n",
    "\n",
    "it takes sometime between 380 and 390 seconds\n",
    "      \n",
    "Whereas the DOK format dict for cosine similarity\n",
    "\n",
    "```python\n",
    "cha_cos_sim_dok = {\n",
    "    (cha_a, cha_b): cosine_similarity(cha_vec_dct[cha_a], cha_vec_dct[cha_b])\n",
    "    for cha_a, cha_b in itertools.combinations_with_replacement(cha_vec_dct.keys(), 2)\n",
    "}\n",
    "```\n",
    "\n",
    "takes 360 seconds.\n",
    "\n",
    "**Every second counts** ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During building the challenges' document vectors (`cha_vec_dct`) and calculating the cosine similarity of every pair of document vectors, I encountered an warning from `numpy` indicating that there are _invalid values in true_divide_. This usually means that somewhere during the calculation there is _zero divided by zero_ happening.\n",
    "\n",
    "I went back to challenge requirement corpus (`challenge_req` DataFrame) and found that there are _empty corpus_ (`''`) and corpus with no meaningful word in it (`any(word for word in corpus if word in trained_wv.vocab) == False`). The results of document vector calculation for these corpora are all **integer 0**, instead of a 200-dimension row vector. \n",
    "\n",
    "And when calculating cosine similarity with two vectors' dot product divided by the product of their norm \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\" Cosine similarity.\"\"\"\n",
    "    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "```\n",
    "\n",
    "There will be `NaN` resulted in `cha_vec_dct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_req = topcoder.corpus.get_challenge_req_sentences(as_dataframe=True)\n",
    "\n",
    "# Remove empty requirement corpora\n",
    "cleaned_challenge_req = challenge_req.loc[challenge_req.requirements != '']\n",
    "\n",
    "# calculate the vector representation of each challenge, store it in a dictionary\n",
    "cha_vec_dct = {cha_id: doc_vector_from_word_vectors(cha['requirements'], trained_wv) for cha_id, cha in cleaned_challenge_req.to_dict(orient='index').items()}\n",
    "\n",
    "# get zero vectors caused by non-empty requirements which don't have any meaningful words\n",
    "zero_vec = {cha_id: vec for cha_id, vec in cha_vec_dct.items() if not isinstance(vec, np.ndarray)}\n",
    "\n",
    "cleaned_cha_vec_dct = {cha_id: vec for cha_id, vec in cha_vec_dct.items() if cha_id not in zero_vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removed_challenge_ids = [*(set(challenge_req.index) - set(cleaned_challenge_req.index)), *list(zero_vec.keys())]\n",
    "print(f'Removed {len(removed_challenge_ids)} challenges which produce invalid vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sec_req = topcoder.corpus.sectioned_requirements\n",
    "sec_req.loc[sec_req.index.get_level_values(1).isin(removed_challenge_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Above DataFrame showcases the challenge requirement corpora that are empty or don't have meaning content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate cosine similarity for every pair of challenges, sotre it in a DOK format dictionary\n",
    "cha_cos_sim_dok = {\n",
    "    (cha_a, cha_b): cosine_similarity(cha_vec_dct[cha_a], cha_vec_dct[cha_b])\n",
    "    for cha_a, cha_b in itertools.combinations_with_replacement(cleaned_cha_vec_dct.keys(), 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instead of directly sorting & selecting from the DOK, I build a DataFrame from it as the pandas implementation outperformed python built-in `dict` A LOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cha_cos_sim_df = pd.DataFrame.from_dict(cha_cos_sim_dok, orient='index')\n",
    "cha_cos_sim_df.index = pd.MultiIndex.from_tuples(cha_cos_sim_df.index)\n",
    "cha_cos_sim_df.index.names, cha_cos_sim_df.columns = ['l0', 'l1'], ['similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_estimated_prize = {}\n",
    "challenge_actual_prize = topcoder.challenge_prize_avg_score.total_prize\n",
    "\n",
    "for cha_id in cleaned_cha_vec_dct:\n",
    "    all_cha_sim = cha_cos_sim_df.loc[(cha_cos_sim_df.index.get_level_values(0) == cha_id) | (cha_cos_sim_df.index.get_level_values(1) == cha_id)]\n",
    "    all_cha_sim.index = all_cha_sim.index.map(lambda ids: ids[0] if ids[0] != cha_id else ids[1])\n",
    "    top10_most_similar_cha = all_cha_sim.similarity.sort_values(ascending=False).iloc[1: 11].index\n",
    "    \n",
    "    challenge_estimated_prize[cha_id] = challenge_actual_prize[challenge_actual_prize.index.isin(top10_most_similar_cha)].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cha_est_prz_df = pd.DataFrame.from_dict(challenge_estimated_prize, orient='index')\n",
    "cha_est_prz_df.columns = ['estimated_total_prize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cha_est_prz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = cha_cos_sim_df.loc[(cha_cos_sim_df.index.get_level_values(0) == 30121730) | (cha_cos_sim_df.index.get_level_values(1) == 30121730)]\n",
    "# s.index = s.index.map(lambda cid_pair: cid_pair[0] if cid_pair[0] != 30121730 else cid_pair[1])\n",
    "# s.similarity.sort_values(ascending=False).iloc[1:11].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing model 0\n",
    "\n",
    "Base on text mining and analogy estimation approach\n",
    "\n",
    "- ✅ calculate Document2Vector of challenges\n",
    "- ✅ calculate similarity between each pair of challenges\n",
    "- for each task\n",
    "  1. select 10 most similar tasks\n",
    "  2. pricing strategies:\n",
    "      - use average prize of 10 tasks as estimate prize of given\n",
    "      - use mid prize of ...\n",
    "  3. calculate estimation error (MRE magnitude of relative error) based on actual prize\n",
    "  4. repeat\n",
    "- on the entrie dataset, calculate mean MRE -> See how big is it, **this is the measure of accuraccy of the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing model 1\n",
    "\n",
    "Once we get the MRE of all task, we can use any machine learning tech to analyze the error. e.g logistic regression\n",
    "\n",
    "each ML approach produce one model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing model 2\n",
    "\n",
    "Take the meta data of challenges/projects into consideration\n",
    "\n",
    "- Types of challenges (aggregation & sub-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "> Nice to have application  \n",
    "> Not only give a estimated price, but also reasons realted to recommended prize considerin the dynamic context.\n",
    "\n",
    "Take a specific vector space, encode the hidden context, find the hidden factors of uncertainty of a given challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_lst = ['java', 'python', 'javascript', 'php', 'mysql', 'api', 'design', 'prototype', 'ui', 'data', 'science']\n",
    "wv_2D = reduce_wv_dimensions(trained_wv)\n",
    "\n",
    "plot_word2vec(wv_2D, label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenv5b62939097214b20b0ca6487fffe5cee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
