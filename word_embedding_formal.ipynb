{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This notebook will be the formal to train, analyze the word embedding data\n",
    "    (with some ugly code temperately existed of course - but will be cleaned eventually!)\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tc_main import TopCoder\n",
    "from tc_pricing_models import train_word2vec_model, reduce_wv_dimensions, plot_word2vec, cosine_similarity, doc_vector_from_word_vectors\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# pricing_model_0/all_track/models/model_200D\n",
    "TRAINED_WV_PATH = os.path.join(os.curdir, 'pricing_model_0', 'all_track', 'models', 'model_200D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topcoder = TopCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment following line if training a new model\n",
    "# trained_wv = train_word2vec_model(sentences=topcoder.corpus.get_challenge_req_sentences(), size=200)\n",
    "\n",
    "# Un-comment following line if using a trained model\n",
    "trained_wv = KeyedVectors.load(TRAINED_WV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_req = topcoder.corpus.get_challenge_req_sentences(as_dataframe=True)\n",
    "\n",
    "# Remove empty requirement corpora\n",
    "cleaned_challenge_req = challenge_req.loc[challenge_req.requirements != '']\n",
    "\n",
    "# calculate the vector representation of each challenge, store it in a dictionary\n",
    "cha_vec_dct = {cha_id: doc_vector_from_word_vectors(cha['requirements'], trained_wv) for cha_id, cha in cleaned_challenge_req.to_dict(orient='index').items()}\n",
    "\n",
    "# get zero vectors caused by non-empty requirements which don't have any meaningful words\n",
    "zero_vec = {cha_id: vec for cha_id, vec in cha_vec_dct.items() if not isinstance(vec, np.ndarray)}\n",
    "\n",
    "cleaned_cha_vec_dct = {cha_id: vec for cha_id, vec in cha_vec_dct.items() if cha_id not in zero_vec}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During building the challenges' document vectors (`cha_vec_dct`) and calculating the cosine similarity of every pair of document vectors, I encountered an warning from `numpy` indicating that there are _invalid values in true_divide_. This usually means that somewhere during the calculation there is _zero divided by zero_ happening.\n",
    "\n",
    "I went back to challenge requirement corpus (`challenge_req` DataFrame) and found that there are _empty corpus_ (`''`) and corpus with no meaningful word in it (`any(word for word in corpus if word in trained_wv.vocab) == False`). The results of document vector calculation for these corpora are all **integer 0**, instead of a 200-dimension row vector. \n",
    "\n",
    "And when calculating cosine similarity with two vectors' dot product divided by the product of their norm \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\" Cosine similarity.\"\"\"\n",
    "    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "```\n",
    "\n",
    "There will be `NaN` resulted in `cha_vec_dct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removed_challenge_ids = [*(set(challenge_req.index) - set(cleaned_challenge_req.index)), *list(zero_vec.keys())]\n",
    "print(f'Removed {len(removed_challenge_ids)} challenges which produce invalid vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_req = topcoder.corpus.sectioned_requirements\n",
    "sec_req.loc[sec_req.index.get_level_values(1).isin(removed_challenge_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Above DataFrame showcases the challenge requirement corpora that are empty or don't have meaning content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Instead of directly sorting & selecting from the DOK, I build a DataFrame from it as the pandas implementation outperformed python built-in `dict` A LOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate cosine similarity for every pair of challenges, sotre it in a DOK format dictionary\n",
    "cha_cos_sim_dok = {\n",
    "    (cha_a, cha_b): cosine_similarity(cha_vec_dct[cha_a], cha_vec_dct[cha_b])\n",
    "    for cha_a, cha_b in itertools.combinations_with_replacement(cleaned_cha_vec_dct.keys(), 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turn DOK into a DataFrame to take advantages of pandas' performance\n",
    "cha_cos_sim_df = pd.DataFrame.from_dict(cha_cos_sim_dok, orient='index')\n",
    "cha_cos_sim_df.index = pd.MultiIndex.from_tuples(cha_cos_sim_df.index)\n",
    "cha_cos_sim_df.index.names, cha_cos_sim_df.columns = ['l0', 'l1'], ['similarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge_estimated_prize = {}\n",
    "# challenge_actual_prize = topcoder.challenge_prize_avg_score.total_prize\n",
    "\n",
    "# for cha_id in cleaned_cha_vec_dct:\n",
    "#     all_cha_sim = cha_cos_sim_df.loc[(cha_cos_sim_df.index.get_level_values(0) == cha_id) | (cha_cos_sim_df.index.get_level_values(1) == cha_id)]\n",
    "#     all_cha_sim.index = all_cha_sim.index.map(lambda ids: ids[0] if ids[0] != cha_id else ids[1])\n",
    "#     top10_most_similar_cha = all_cha_sim.similarity.sort_values(ascending=False).iloc[1: 11].index\n",
    "    \n",
    "#     challenge_estimated_prize[cha_id] = challenge_actual_prize[challenge_actual_prize.index.isin(top10_most_similar_cha)].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cha_est_prz_df = pd.DataFrame.from_dict(challenge_estimated_prize, orient='index')\n",
    "# cha_est_prz_df.columns = ['estimated_total_prize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricing_model_measurement_df = cha_est_prz_df.join(challenge_actual_prize)\n",
    "# pricing_model_measurement_df = pricing_model_measurement_df.loc[pricing_model_measurement_df.total_prize != 0]\n",
    "# pricing_model_measurement_df['MRE'] = (pricing_model_measurement_df.total_prize - pricing_model_measurement_df.estimated_total_prize).abs() / pricing_model_measurement_df.total_prize\n",
    "\n",
    "# pricing_model_measurement_df.MRE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('test.json', 'w') as f:\n",
    "#     pricing_model_measurement_df.reset_index().to_json(f, orient='records', indent=4, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = cha_cos_sim_df.loc[(cha_cos_sim_df.index.get_level_values(0) == 30121730) | (cha_cos_sim_df.index.get_level_values(1) == 30121730)]\n",
    "# s.index = s.index.map(lambda cid_pair: cid_pair[0] if cid_pair[0] != 30121730 else cid_pair[1])\n",
    "# s.similarity.sort_values(ascending=False).iloc[1:11].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing model 0\n",
    "\n",
    "Base on text mining and analogy estimation approach\n",
    "\n",
    "- âœ… calculate Document2Vector of challenges\n",
    "- âœ… calculate similarity between each pair of challenges\n",
    "- âœ…for each task\n",
    "  1. select 10 most similar tasks\n",
    "  2. pricing strategies:\n",
    "      - use average prize of 10 tasks as estimate prize of given\n",
    "      - use mid prize of ...\n",
    "  3. calculate estimation error (MRE magnitude of relative error) based on actual prize\n",
    "  4. repeat\n",
    "- âœ…on the entrie dataset, calculate mean MRE -> See how big is it, **this is the measure of accuraccy of the model**\n",
    "\n",
    "The pricing model 0's mean MRE is **5.248539251177451**. Tragic ðŸ¤¦ðŸ»â€â™‚ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing model 1\n",
    "\n",
    "Once we get the MRE of all task, we can use any machine learning tech to analyze the error. e.g logistic regression\n",
    "\n",
    "each ML approach produce one model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing model 2\n",
    "\n",
    "Take the meta data of challenges/projects into consideration\n",
    "\n",
    "- Types of challenges (aggregation & sub-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "> Nice to have application  \n",
    "> Not only give a estimated price, but also reasons realted to recommended prize considerin the dynamic context.\n",
    "\n",
    "Take a specific vector space, encode the hidden context, find the hidden factors of uncertainty of a given challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.curdir, 'data', 'tech_by_start_date.json')) as f:\n",
    "    tech_by_start_dt = json.load(f)\n",
    "df_tech_by_dt = pd.DataFrame.from_dict(tech_by_start_dt, orient='index').fillna(0).astype(int).drop(columns='other')\n",
    "df_tech_by_dt.index = pd.to_datetime(df_tech_by_dt.index)\n",
    "df_tech_by_dt.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tech_by_dt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# label_lst = ['java', 'python', 'javascript', 'php', 'mysql', 'api', 'design', 'prototype', 'ui', 'data', 'science']\n",
    "wv_2D = reduce_wv_dimensions(trained_wv)\n",
    "\n",
    "# plot_word2vec(wv_2D, list(df_tech_by_dt.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_lst = [t for t in df_tech_by_dt.columns if 2 < len(t) < 20]\n",
    "wv_2D['is_tech'] = wv_2D.word.isin(tech_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), dpi=200)\n",
    "\n",
    "with sns.axes_style('darkgrid'):\n",
    "    ax = fig.add_axes([0.2, 0.2, 0.8, 0.8])\n",
    "    sns.scatterplot(\n",
    "        data=wv_2D, \n",
    "        x='x', \n",
    "        y='y',\n",
    "        hue='is_tech',\n",
    "        alpha=0.5,\n",
    "        palette=['#8a8a8a', '#FF0000'],\n",
    "        size=1,\n",
    "        linewidth=0.2,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('All track 200D word vectors after dimension reduction')\n",
    "    ax.title.set_color('white')\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')\n",
    "    ax.tick_params(axis='both', colors='white')\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[:-1], labels[:-1], prop={'size': 8})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenv5b62939097214b20b0ca6487fffe5cee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
