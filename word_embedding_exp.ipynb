{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding and requirement analysis\n",
    "\n",
    "This notebook is created for performing word embedding analysis and other text data processing for TopCoder's challenges requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.curdir, 'data', 'detail_requirements.json')) as f:\n",
    "    detailed_req = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_txt_from_node(node, is_nav=False, delimiter=' '):\n",
    "    \"\"\" Extract text and lowercase the text then unify the white space.\"\"\"\n",
    "    url_regex = r'(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)'\n",
    "\n",
    "    reg = re.compile(url_regex)\n",
    "    text = node.strip() if is_nav else node.get_text()\n",
    "\n",
    "    return delimiter.join(reg.sub('', text).lower().split())\n",
    "\n",
    "\n",
    "def sectionlize_requirements(req):\n",
    "    \"\"\" Aggregate the requirement paragraph by header tag. \"\"\"\n",
    "    sectioned_req_dct = defaultdict(list)\n",
    "    soup = BeautifulSoup(req, 'html.parser')\n",
    "    \n",
    "    if soup.a:\n",
    "        soup.a.decompose()\n",
    "    if soup.img:\n",
    "        soup.img.decompose()\n",
    "    \n",
    "    all_header_tags = soup.find_all(re.compile(r'^h'))\n",
    "    \n",
    "    if len(all_header_tags) == 0:\n",
    "        return {'no_header_tag': extract_txt_from_node(soup)}\n",
    "    \n",
    "    for header in all_header_tags:\n",
    "        section_name = extract_txt_from_node(header, delimiter='_')\n",
    "        nxt_node = header\n",
    "        while True:\n",
    "            nxt_node = nxt_node.nextSibling\n",
    "            \n",
    "            if nxt_node is None:\n",
    "                break\n",
    "                \n",
    "            if isinstance(nxt_node, NavigableString):\n",
    "                sectioned_req_dct[section_name].append(extract_txt_from_node(nxt_node, is_nav=True))\n",
    "            if isinstance(nxt_node, Tag):\n",
    "                if nxt_node.name.startswith('h'):\n",
    "                    break\n",
    "                sectioned_req_dct[section_name].append(extract_txt_from_node(nxt_node))\n",
    "    \n",
    "    return {sec_name: ' '.join(' '.join(sec_reqs).split()) for sec_name, sec_reqs in sectioned_req_dct.items()}\n",
    "\n",
    "def get_similarity_score(lst_of_str):\n",
    "    \"\"\" Calculate the simliarity scroe from a list of strings\"\"\"\n",
    "    seq_matcher = difflib.SequenceMatcher()\n",
    "    similarity_score_sum = 0\n",
    "    \n",
    "    for idx, s in enumerate(lst_of_str[:-1]):\n",
    "        seq_matcher.set_seq2(s)\n",
    "        for s1 in lst_of_str[idx + 1:]:\n",
    "            seq_matcher.set_seq1(s1)\n",
    "            similarity_score_sum += round(seq_matcher.ratio(), 3)\n",
    "            \n",
    "    return round(similarity_score_sum / ((len(lst_of_str) * (len(lst_of_str) - 1)) / 2), 3)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectionlize the requirements\n",
    "\n",
    "By manually going through parts of the requirement text, I found that some of the challenges have the requirements separated into different sections. Some of the commonly used sections are **Project/Challenge Overview**, **Technology Stacks**, etc.\n",
    "\n",
    "This finding indicated that there could be a possibility that **there is a degree of redundency/overlapping among quirement description of challenges under the same project**. To verify this assumption. The following step are performed.\n",
    "\n",
    "1. The detailed requirements for TopCoder challenges are in the format of HTML, where the header tags (`<h1>/<h2>/<h3>...`) mark out the different sections in the requirement text. When using BeautifulSoup for text extraction, I first found all of the header tags in the document, then use the header tags as separator the divide the document in different sections. These section text are store in a data structure of dictionary with the header text as keys. (The code can be found in function `sectionlize_requirements` above)\n",
    "\n",
    "2. For every challenge in each project, we perform the text sectionlization. The result is a nested dictionaries with `project_id` as first level keys and `challenge_id` as second level keys:\n",
    "\n",
    "   ```python\n",
    "   {'project_id': {\n",
    "       'challenge_id_0': {\n",
    "           'title': title_text,\n",
    "           'requirements': {\n",
    "               'header_0': text_of_the_section,\n",
    "               'header_1': text_of_the_section,...\n",
    "           },\n",
    "       },...\n",
    "   }\n",
    "        \n",
    "   ```\n",
    "   \n",
    "   (code can be found in the next cell)\n",
    "   \n",
    "3. For each project, find the common section headers among the challenges. **We will detail this step in following chunks of code. We did it in two similar but different ways**. The header `no_header_tag` is omitted on purpose - this header means there was no header found when sectionlizing the text. The text paragraphes under the common sections in the challenges are grouped into one list. A new dictionary with following shape is formed therefore:\n",
    "\n",
    "   ```python\n",
    "   {\n",
    "       'project_id_0': {\n",
    "           'section_name_0': [list_of_text_from_challenges],\n",
    "           'section_name_1': [list_of_text_from_challenges],...\n",
    "       },\n",
    "       'project_id_1':...\n",
    "   }\n",
    "   ```\n",
    "   \n",
    "4. For the list of text in the dictionary above, a unique combination of two elements is picked out and the similarity of text are calculated. **Currently, the built-in python package `difflib` is used for similarity calculation, an alternative can be [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)**. The calculation is performed recursively for each unique combination of two in the list. Then the average of all similarity scores are taken as the final similiarity score for the section.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_req = defaultdict(dict)\n",
    "\n",
    "for dr in detailed_req:\n",
    "    processed_req[dr['project_id']][dr['challenge_id']] = {\n",
    "        'title': ' '.join(dr['title'].lower().split()),\n",
    "        'requirements': sectionlize_requirements(dr['requirements'])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processed_req_for_df = {\n",
    "    (project_id, challenge_id, sec_name): {'sectioned_requirement': sec_text}\n",
    "    for project_id, challenges in processed_req.items() \n",
    "    for challenge_id, dr in challenges.items() \n",
    "    for sec_name, sec_text in dr['requirements'].items()\n",
    "}\n",
    "\n",
    "df_requirements = pd.DataFrame.from_dict(processed_req_for_df, orient='index')\n",
    "df_requirements.index.names = ['project_id', 'challenge_id', 'section_name']\n",
    "# df_requirements.tail(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find common section names - attempt 0\n",
    "\n",
    "The first attempt for find the common section names is straightforward: the section names appear in **every challenge** is considered common section names. \n",
    "\n",
    "This approach is simply but has one problem - **For a project with 10 challenges, if a section name appears in 9 of the challenges instead of 10, it's _not_ considered as a \"common section names.** Shown below, the project `12862` has 12 challenges, 11 of which have a section `general_note` but this section is not picked out as a common section name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_by_proj = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for project_id, challenges in processed_req.items():\n",
    "    common_section_names = set.intersection(*[set(challenge['requirements'].keys()) for challenge in challenges.values()])\n",
    "    \n",
    "    for section_name in common_section_names:\n",
    "        if section_name != 'no_header_tag':\n",
    "            sections_by_proj[project_id][section_name] = [challenge['requirements'][section_name] for challenge in challenges.values()]\n",
    "\n",
    "section_similarity_score = defaultdict(dict)\n",
    "\n",
    "for project_id, requirement_section in sections_by_proj.items():\n",
    "    for sec_name, lst_of_requirements in requirement_section.items():\n",
    "        section_similarity_score[project_id][sec_name] = get_similarity_score(lst_of_requirements)\n",
    "        \n",
    "df_similarity_score = pd.DataFrame.from_dict(\n",
    "    data={(project_id, section_name): {'score': score} for project_id, sections in section_similarity_score.items() for section_name, score in sections.items()},\n",
    "    orient='index'\n",
    ")\n",
    "df_similarity_score.index.names = ['project_id', 'section_name']\n",
    "print(f'{len(df_similarity_score)} common sections found')\n",
    "df_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_requirements.loc[[12862]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find common section names - attempt 1\n",
    "\n",
    "The second attempt for finding common section names consider a section name as common section name when the section name appears more than a given ferquency in challenges under the project.\n",
    "\n",
    "Despite of the tragic performance of the code snippet(takes almost 5 minutes to finish all the for loop), the new approach found 263 \"common section names\" in the projects with a threshold of 0.5, meaning that the section name appears in more than half of the challenges under the same project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5 # a section name is considered \"common\" if its frequency of apperance greater than the threshold\n",
    "sections_by_proj = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for project_id, challenges in processed_req.items():\n",
    "    sec_name_freqency = defaultdict(int)\n",
    "    number_of_challenges = len(challenges)\n",
    "    \n",
    "    for challenge in challenges.values():\n",
    "        for sec_name in challenge['requirements']:\n",
    "            if sec_name != 'no_header_tag':\n",
    "                sec_name_freqency[sec_name] += 1\n",
    "            \n",
    "    for sec_name, sec_name_freq in sec_name_freqency.items():\n",
    "        if sec_name_freq / number_of_challenges > THRESHOLD:\n",
    "            sections_by_proj[project_id][sec_name] = {\n",
    "                'section_name_frequency': round(sec_name_freq / number_of_challenges, 2),\n",
    "                'lst_of_requirements': [challenge['requirements'][sec_name] for challenge in challenges.values() if sec_name in challenge['requirements']]\n",
    "            }\n",
    "            \n",
    "section_similarity_score = defaultdict(lambda: defaultdict(dict))\n",
    "for project_id, requirement_section in sections_by_proj.items():\n",
    "    for sec_name, section in requirement_section.items():\n",
    "        section_similarity_score[project_id][sec_name] = {\n",
    "            'score': get_similarity_score(section['lst_of_requirements']),\n",
    "            'apperance_frequency': section['section_name_frequency']\n",
    "        }\n",
    "        \n",
    "df_similarity_score = pd.DataFrame.from_dict(\n",
    "    data={(project_id, section_name): sec for project_id, sections in section_similarity_score.items() for section_name, sec in sections.items()},\n",
    "    orient='index'\n",
    ")\n",
    "df_similarity_score.index.names = ['project_id', 'section_name']\n",
    "print(f'{len(df_similarity_score)} common sections found')\n",
    "df_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Analysis)",
   "language": "python",
   "name": "data_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}